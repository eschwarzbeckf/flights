{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15333285-ab3f-47b5-b2f8-926557b104f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "HADOOP_AWS_PACKAGE = \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "AWS_JAVA_SDK_PACKAGE = \"com.amazonaws:aws-java-sdk-bundle:1.12.367\"\n",
    "MARIADB_JAR_PACKAGE = \"org.mariadb.jdbc:mariadb-java-client:3.2.0\"\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Flights\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"8nf54yOno6QaSNgjTKQC\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"bpD8MH2glh4vZ5Mr0kYlfhpJTTNv8bglKr74pW2U\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.jars.packages\", f\"{HADOOP_AWS_PACKAGE},{AWS_JAVA_SDK_PACKAGE},{MARIADB_JAR_PACKAGE}\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "minio_path = \"s3a://flights/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d107ecf1-8f55-46e5-8646-734101be1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mariadb(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Writes a micro-batch DataFrame to a MariaDB table.\n",
    "    This function is executed on a Spark executor for each batch.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing Batch ID: {batch_id} ---\")\n",
    "    \n",
    "    # Caching the DataFrame can sometimes help with performance and reliability.\n",
    "    batch_df.persist()\n",
    "    \n",
    "    # Ensure the dataframe is not empty to avoid creating unnecessary connections.\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch {batch_id} is empty. Skipping write.\")\n",
    "        batch_df.unpersist()\n",
    "        return\n",
    "\n",
    "    print(f\"Batch {batch_id} contains {batch_df.count()} rows to write.\")\n",
    "\n",
    "    # JDBC connection properties.\n",
    "    # The key change is adding '?sessionVariables=sql_mode=ANSI_QUOTES' to the URL.\n",
    "    # This tells MariaDB to treat double-quoted strings as identifiers, which matches Spark's default behavior.\n",
    "    jdbc_url = \"jdbc:mariadb://db:3306/airplanes?sessionVariables=sql_mode=ANSI_QUOTES\"\n",
    "    jdbc_properties = {\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.mariadb.jdbc.Driver\"\n",
    "    }\n",
    "    \n",
    "    # Write the batch DataFrame to MariaDB.\n",
    "    try:\n",
    "        batch_df.write \\\n",
    "            .jdbc(url=jdbc_url, table=\"curr_flights\", mode=\"overwrite\", properties=jdbc_properties)\n",
    "        print(f\"Successfully wrote batch {batch_id} to MariaDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to write batch {batch_id} to MariaDB.\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        # Re-raising the exception will cause the streaming query to fail.\n",
    "        # You might want to handle this differently, e.g., write to a dead-letter queue.\n",
    "        raise e\n",
    "    finally:\n",
    "        # Unpersist the DataFrame to free up memory.\n",
    "        batch_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697bb31e-adfd-4d4d-b6c2-089a68a366f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "        .csv(minio_path, header=True, inferSchema=True)\n",
    "\n",
    "flights_schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5949958b-76a9-4415-8329-668383f6404e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss = spark.readStream\\\n",
    "    .format(\"csv\")\\\n",
    "    .schema(flights_schema)\\\n",
    "    .option(\"header\",True)\\\n",
    "    .option(\"maxFilesPerTrigger\",1)\\\n",
    "    .option(\"latestFirst\",True)\\\n",
    "    .option(\"cleanSource\",\"delete\")\\\n",
    "    .load(minio_path)\n",
    "\n",
    "ss_filter = ss.filter((ss.on_ground == False))\\\n",
    "            .filter(ss.callsign.isNotNull())\\\n",
    "            .filter(ss.latitude.isNotNull())\\\n",
    "            .filter(ss.longitude.isNotNull())\\\n",
    "            .withColumns({\n",
    "                'last_contact':F.from_unixtime(ss.last_contact,'yyyy-MM-dd HH:mm:ss').cast(TimestampType()),\n",
    "                'time_position':F.from_unixtime(ss.time_position,'yyyy-MM-dd HH:mm:ss').cast(TimestampType())\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21423a-5a13-4270-ba90-7421c5e6d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=ss_filter.writeStream\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .foreachBatch(write_to_mariadb)\\\n",
    "    .trigger(processingTime='3 seconds')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
